--- python src/model_talk_topics.py GMF
Optimization Complete: Convergence on objective within bounds.
Computing final objective value and training RMSE.
Final objective value: 0.00729485
Final training RMSE: 0.0854099  

--- python src/model_user_groups.py


--- python src/recommend_talks.py EVALUATE (n_wider=2, n_keeper=2)
LDA
2016/11/18 14:12:34: 
Evaluation Results for model LDA
My recommender: deeper 0.381865, wider 0.395333, total 0.777198
Benchmark: deeper 0.360840, wider 0.355900, total 0.716740
outputform: score 0.0605, freq 0.679333, pvalue 0.000000

NMF
2016/11/18 14:15:26: 
Evaluation Results for model NMF
My recommender: deeper 0.478411, wider 0.285879, total 0.764290
Benchmark: deeper 0.538399, wider 0.343283, total 0.881681
outputform: score -0.1174, freq 0.325333, pvalue 0.000000

GMF
2016/11/20 23:03:35: 
Evaluation Results for model GMF
My recommender: deeper 0.199184, wider 0.210224, total 0.409408
Benchmark: deeper 0.200919, wider 0.198319, total 0.399238
outputform: score 0.0102, freq 0.533333, pvalue 0.061573



--- rec "Informative, Inspiring" + "internet technology data computer investment finance economics"
=====Bill Gross: The single biggest reason why startups succeed (TED2015)=====
[keywords]
business, creativity, technology
=====Dilip Ratha: The hidden force in global economics: sending money home (TEDGlobal 2014)=====
[keywords]
business, economics, finance
=====Joi Ito: Want to innovate? Become a "now-ist" (TED2014)=====
[keywords]
business, creativity, design, engineering, entrepreneur, innovation, investment, natural disaster,
nuclear energy, product design, science, technology, telecom
=====Sandra Aamodt: Why dieting doesn't usually work (TEDGlobal 2013)=====
[keywords]
health, neuroscience, obesity

-- sep, 2012
Talks: 1203
Speakers: 1006
Users: 74760
Active Users: 12605
Tags: 298
Themes: 46
Transcripts: 1203
Related Videos: 3090
Favorites: 134533
Comments: 209566
--

m users, n talks, k latent features
create rating_df = (user_id, fav_talk_id) data
run matrix factorization based on rating_df, U x V
for each user "ui", rank k latent features
define peers "pi" for the user "ui" as users with same top 2 latent features
calculate average rank for each k-2 latent features
define potential learning topic "Li" as the latent features with highest average rankings
define potential interested talks "ti" as top 10 talks for latent feature "Li" from V
calculate the rating distance between favorite talks and the potential interested talks
find the recommended talk "ri" with smallest distance

ui: user i
pi: peers for user i
Li: potential most interested learning topic
ti: potential most interested talks of Li
ri: recommended talk for user i


